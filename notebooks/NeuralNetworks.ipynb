{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1581509a",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09554e94",
   "metadata": {},
   "source": [
    "## Import needed libraries:\n",
    "* numpy\n",
    "* tqdm\n",
    "* skelarn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3fede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25266efd",
   "metadata": {},
   "source": [
    "### sigmoid():\n",
    "The sigmoid function.\n",
    "\n",
    "### d_sigmoid():\n",
    "The derivative of sigmoid function.\n",
    "\n",
    "### leakyReLu():\n",
    "The leaky relu function.\n",
    "\n",
    "### d_leakyReLu():\n",
    "The derivative of leaky relu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ca014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    # avoid overflow in exp function\n",
    "    Z = np.clip( Z, -600, 600 )\n",
    "    return 1.0 / (1 + np.exp(-Z))\n",
    "    \n",
    "def d_sigmoid(Z):\n",
    "    s = sigmoid(Z)\n",
    "    return s*(1-s)\n",
    "\n",
    "def leakyReLu(Z):\n",
    "    Z[Z<0]=0.01*Z[Z<0]\n",
    "    return Z\n",
    "\n",
    "def d_leakyReLu(Z):\n",
    "    Z[Z<0]=0.01\n",
    "    Z[Z>0]=1\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc77865",
   "metadata": {},
   "source": [
    "## Neural Network Model\n",
    "\n",
    "## <span style=\"color:green\">class</span>  <span style=\"color:blue\">Layer</span>:\n",
    "\n",
    "### forward_prop():\n",
    "Implements the forward propagation for the current layer, using its weights, te bias term and the given input vector.\n",
    "\n",
    "### back_prop():\n",
    "Implements the back propagation(calculating the gradients) for the current layer using the upper layer's parameters and the given input vector.\n",
    "<br />\n",
    "Also, _lambda parameter is passed for L2 regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, units, weights, activation='sigmoid'):\n",
    "        np.random.seed(42)\n",
    "        initialization_factor = np.sqrt(1/(weights-1)) if weights > 1 else 0.01\n",
    "        self.W = np.random.randn(units, weights) * initialization_factor\n",
    "        self.b = np.zeros((units, 1))\n",
    "        if(activation=='sigmoid'):\n",
    "            self.activation=sigmoid\n",
    "            self.d_activation=d_sigmoid\n",
    "        else:\n",
    "            self.activation=leakyReLu\n",
    "            self.d_activation=d_leakyReLu\n",
    "         \n",
    "    def forward_prop(self, A_previous):\n",
    "        self.Z = np.dot(self.W, A_previous) + self.b\n",
    "        self.A = self.activation(self.Z)\n",
    "        return self.A\n",
    "        \n",
    "    def back_prop(self, upper_layer, A_previous, m, learning_rate, _lambda):\n",
    "        regularization_term = _lambda * self.W\n",
    "        \n",
    "        # compute gradients\n",
    "        self.dZ = upper_layer.W.T.dot(upper_layer.dZ) * self.d_activation(self.Z)\n",
    "        self.dW = (1/m) * (self.dZ.dot(A_previous.T) - regularization_term)\n",
    "        self.db = (1/m) * np.sum(self.dZ, axis=1, keepdims=True)\n",
    "        # update weights\n",
    "        self.W -= learning_rate*self.dW\n",
    "        self.b -= learning_rate*self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbfb737",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">class</span>  <span style=\"color:blue\">NeuralNetwork</span>:\n",
    "\n",
    "### input_layer():\n",
    "The input of the model:\n",
    "* X: The training vector. Should be passed as X.shape=(m,n)\n",
    "* y: The corresponding labels. Should be passed as y.shape=(m,1)\n",
    "\n",
    "### add_layer():\n",
    "Call for adding a new hidden layer or the output layer.\n",
    "<br />\n",
    "The input is an instance of <span style=\"color:green\">class</span>  <span style=\"color:blue\">Layer</span>.\n",
    "\n",
    "### __forward_prop():\n",
    "* Implements the forward propagation for the network over all layers and returns the y_hat-predictions of the training set.\n",
    "<br />\n",
    "* Calls **Layer.forward_prop()** for each layer.\n",
    "\n",
    "### __back_prop():\n",
    "* Implements the back propagation for the network over all layers, computes the gradients for each layer's weights and bias term and updates them using gradient descent with the given learning_rate and _lambda for L2 regularization.\n",
    "<br />\n",
    "* Calls **Layer.back_prop()** for each hidden layer.\n",
    "<br />\n",
    "* The gradients update for the output_layer is implemented in the __back_prop(), because of its different derivative computation.\n",
    "\n",
    "### fit() :\n",
    "Call to train the model. \n",
    "<br />\n",
    "* If early_stopping arguement is True, the algorithm will stop training the model if the mean dev cost is increasing for 5 consecutive epochs or if it reaches the limit of the given epochs arguement.\n",
    "* If early_stopping arguement is True the algorithm will iterate over the training examples for the given number of epochs.\n",
    "\n",
    "### predict():\n",
    "Predicts the output (0 or 1) of a given dataset X.\n",
    "\n",
    "### accuracy():\n",
    "Predicts the accuracy of the model on a given dataset X and its output y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.J = np.array([])\n",
    "        \n",
    "    def input_layer(self, X, y):\n",
    "        self.X = X.T\n",
    "        self.y = y\n",
    "        self.m, self.n = X.shape\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def __forward_prop(self, X, Y):\n",
    "        A_previous = X\n",
    "        for layer in self.layers:\n",
    "            A_previous = layer.forward_prop(A_previous)\n",
    "            \n",
    "        y_hat = A_previous\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    def __back_prop(self, X, y, y_hat, learning_rate, _lambda):\n",
    "        #update output layer\n",
    "        regularization_term = _lambda * self.layers[-1].W\n",
    "        \n",
    "        self.layers[-1].dZ = y_hat - y\n",
    "        if(len(self.layers) == 1):\n",
    "            self.layers[-1].dW = (1/self.m) * self.layers[-1].dZ.dot(X.T) - regularization_term\n",
    "        else:\n",
    "            self.layers[-1].dW = (1/self.m) * self.layers[-1].dZ.dot(self.layers[-2].A.T) - regularization_term\n",
    "        \n",
    "        self.layers[-1].db = (1/self.m) * np.sum(self.layers[-1].dZ, axis=1, keepdims=True)\n",
    "        \n",
    "        self.layers[-1].W -=  learning_rate*self.layers[-1].dW\n",
    "        self.layers[-1].b -=  learning_rate*self.layers[-1].db\n",
    "        \n",
    "        if(len(self.layers) == 1):\n",
    "            return\n",
    "        \n",
    "        #update hidden layers except the first hidden layer\n",
    "        for i in range(len(self.layers)-2, 0, -1):\n",
    "            self.layers[i].back_prop(self.layers[i+1], self.layers[i-1].A, X.shape[1], learning_rate, _lambda)\n",
    "        \n",
    "        #update the first hidden layer\n",
    "        self.layers[0].back_prop(self.layers[1], X, X.shape[1], learning_rate, _lambda)\n",
    "        \n",
    "    def fit(self, epochs=100, learning_rate=0.1, decay_rate=0.0, use_tqdm=True, early_stopping = False, X_dev=None, y_dev=None, _lambda=0.1, B=128):\n",
    "        try:\n",
    "            self.epochs = 0\n",
    "            \n",
    "            #total mini batches\n",
    "            batches = self.X.shape[1] // B\n",
    "             \n",
    "            for epoch in tqdm(range(epochs)) if use_tqdm else range(epochs):\n",
    "                for batch in range(batches):\n",
    "                    X = self.X[:,batch*B:(batch+1)*B]\n",
    "                    y = self.y[batch*B:(batch+1)*B]\n",
    "                        \n",
    "                    y_hat = self.__forward_prop(X, y)\n",
    "                    self.__back_prop(X, y, y_hat, learning_rate, _lambda)\n",
    "                        \n",
    "                if (batch+1)*B < self.X.shape[1]:\n",
    "                    y_hat = self.__forward_prop(self.X[:,(batch+1)*B:], self.y[(batch+1)*B:])\n",
    "                    self.__back_prop(self.X[:,(batch+1)*B:], self.y[(batch+1)*B:], y_hat, learning_rate, _lambda)\n",
    "                        \n",
    "                # compute loss at dev data\n",
    "                y_hat_dev = self.__forward_prop(X_dev.T, y_dev)\n",
    "                cost = log_loss(y_dev.reshape(-1), y_hat_dev.reshape(-1))/X_dev.shape[0]\n",
    "                self.J = np.append(self.J, cost)\n",
    "                    \n",
    "                self.epochs+=1\n",
    "                #decrease learning rate in each iterationn if decay_\n",
    "                learning_rate=learning_rate*(1/(1+decay_rate*epoch))\n",
    "                \n",
    "                # train for at least 10 epochs\n",
    "                if self.epochs > 10 and early_stopping==True:\n",
    "                    # early stop if cost is increasing for 5 epochs\n",
    "                    if all(self.J[-1] >= self.J[-6:-1]):\n",
    "                        break\n",
    "            \n",
    "        except:\n",
    "            return\n",
    "    \n",
    "    \n",
    "    def predict(self, X, y):\n",
    "        predictions = self.__forward_prop(X.T, y)\n",
    "        predictions = (predictions >= (0.5 + 1e-6)).astype(int)\n",
    "    \n",
    "        return predictions.reshape(-1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        preds = self.predict(X, y)\n",
    "        accuracy = sum(preds == y) / y.shape[0]\n",
    "        \n",
    "        return accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
